import copy
import logging
import re
import time

import gevent

import locust.env
import locust.log
import locust.stats

import tabulate


def run_locust(args, status_data, test_set, new_stats=False, summary_only=False):
    # Local runner is True by default, bot overwrite it if we have selected
    # master or worker runner
    assert not (
        args.locust_master_runner and args.locust_worker_runner
    ), "Either choose master or worker runner, not both"
    if args.locust_master_runner or args.locust_worker_runner:
        args.locust_local_runner = False

    # Additional opts, maybe put them into args someday
    args.master_port = 5557
    args.master_bind_host = "*"
    args.master_bind_port = 5557
    args.reset_stats = False

    # Add parameters to status data file
    status_data.set("parameters.locust.hatch_rate", args.hatch_rate)
    status_data.set("parameters.locust.host", args.host)
    status_data.set("parameters.locust.num_clients", args.num_clients)
    status_data.set("parameters.locust.reset_stats", args.reset_stats)
    status_data.set("parameters.locust.stop_timeout", args.stop_timeout)
    status_data.set("parameters.test.duration", args.test_duration)
    status_data.set("parameters.test.requests", args.test_requests)
    status_data.set("parameters.test.url_suffix", args.test_url_suffix)
    if "test_selection" in args:
        status_data.set("parameters.test.test_selection", args.test_selection)
    print(
        f"Running with host = {args.host}, num_clients = {args.num_clients}, hatch_rate = {args.hatch_rate}, duration = {args.test_duration} / requests = {args.test_requests}"
    )

    env = locust.env.Environment()
    env.user_classes = [test_set]
    env.stop_timeout = args.stop_timeout
    env.host = args.host
    env.reset_stats = args.reset_stats

    # Create runner and run test
    if args.locust_local_runner:

        env.create_local_runner()
        status_data.set("parameters.locust.runner", "local")

        # Start the test
        logging.info("Starting local Locust runner")
        status_data.set_now("parameters.start")
        env.runner.start(args.num_clients, spawn_rate=args.hatch_rate)

        # Wait (in some way) for a good time to quit the test
        if args.test_requests > 0:
            while True:
                num_requests = env.stats.num_requests
                if num_requests >= args.test_requests:
                    logging.debug(
                        f"Finished {num_requests} requests while requested number was {args.test_requests}"
                    )
                    break
                logging.debug(
                    f"Still waiting for test requests count ({num_requests} out of {args.test_requests})"
                )
                time.sleep(1)
        else:
            time.sleep(args.test_duration)
            logging.debug(f"Waited for {args.test_duration} seconds")
        gevent.spawn(lambda: env.runner.quit())

        # Wait for the greenlets to finish
        env.runner.greenlet.join()
        status_data.set_now("results.end")
        logging.info("Local Locust run finished")

        return show_locust_stats(env.stats, status_data, new_stats, summary_only)

    elif args.locust_master_runner:

        env.create_master_runner(
            master_bind_host=args.master_bind_host,
            master_bind_port=args.master_bind_port,
        )
        status_data.set("parameters.locust.runner", "master")
        status_data.set("parameters.locust.expect_workers", args.expect_workers)

        env.runner.spawn_rate = args.hatch_rate

        while len(env.runner.clients.ready) < args.expect_workers:
            logging.info(
                "Waiting for worker to become ready, %s of %s - %s",
                len(env.runner.clients.ready),
                args.expect_workers,
                ",".join([i.state for i in env.runner.clients.values()]),
            )
            time.sleep(1)

        # Start the test
        logging.info("Starting master Locust runer")
        status_data.set_now("parameters.start")
        env.runner.start(args.num_clients, spawn_rate=args.hatch_rate)

        # Wait configured time and quit the test
        time.sleep(args.test_duration)
        gevent.spawn(lambda: env.runner.quit())

        # Wait for the greenlets to finish
        env.runner.greenlet.join()
        status_data.set_now("results.end")
        logging.info("Master Locust run finished")

        return show_locust_stats(env.stats, status_data, new_stats, summary_only)

    elif args.locust_worker_runner:

        env.create_worker_runner(
            master_host=args.master_host,
            master_port=args.master_port,
        )
        status_data.set("parameters.locust.runner", "worker")
        status_data.set("parameters.locust.master_host", args.master_host)
        status_data.set("parameters.locust.master_port", args.master_port)

        # Start the test
        logging.info("Starting worker Locust runner")
        status_data.set_now("parameters.start")

        # Wait for the greenlets to finish
        env.runner.greenlet.join()
        status_data.set_now("results.end")
        logging.info("Worker Locust run finished")

    else:

        raise Exception("No runner specified")


def show_locust_stats(locust_stats, status_data, new_stats, summary_only):
    """
    Print Locust stats obejct and format nice table of it.
    Also add values to status data object.
    """

    # What is in `value` variable below:
    # ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cache_response_times', '_log_response_time', '_log_time_of_request', 'avg_content_length', 'avg_response_time', 'current_fail_per_sec', 'current_rps', 'extend', 'fail_ratio', 'get_current_response_time_percentile', 'get_response_time_percentile', 'get_stripped_report', 'last_request_timestamp', 'log', 'log_error', 'max_response_time', 'median_response_time', 'method', 'min_response_time', 'name', 'num_fail_per_sec', 'num_failures', 'num_none_requests', 'num_reqs_per_sec', 'num_requests', 'percentile', 'reset', 'response_times', 'response_times_cache', 'serialize', 'start_time', 'stats', 'to_string', 'total_content_length', 'total_fail_per_sec', 'total_response_time', 'total_rps', 'unserialize', 'use_response_times_cache']

    # Format result table
    data = {
        "request": [],
        "count": [],
        "fail ratio": [],
        "med resp time": [],
        "avg content lenght": [],
        "total RPS": [],
    }
    data_new = {}

    # Load all rows
    sum_count = 0
    sum_failures = 0
    sum_total_response_time = 0.0
    sum_total_content_length = 0
    sum_total_rps = 0.0
    for name, value in locust_stats.entries.items():
        sum_count += value.num_requests
        sum_failures += value.num_failures
        sum_total_response_time += value.median_response_time * value.num_requests
        sum_total_content_length += value.total_content_length
        sum_total_rps += value.total_rps
        n = f"{name[1]} {name[0]}"
        if len(n) > 40:
            n = n[:39] + "â€¦"
        data["request"].append(n)
        data["count"].append(value.num_requests)
        data["fail ratio"].append(value.fail_ratio)
        data["med resp time"].append(value.median_response_time)
        data["avg content lenght"].append(
            value.total_content_length / value.num_requests
        )
        data["total RPS"].append(value.total_rps)

        name_safe = re.sub("[^a-zA-Z0-9-]+", "_", f"{name[1]} {name[0]}")
        while name_safe in data_new:
            name_safe += "_"
        data_new[name_safe] = {
            "50percentile": value.get_response_time_percentile(0.50),
            "90percentile": value.get_response_time_percentile(0.90),
            "95percentile": value.get_response_time_percentile(0.95),
            "99percentile": value.get_response_time_percentile(0.99),
            "avg_content_length": value.avg_content_length,
            "avg_response_time": value.avg_response_time,
            "fail_ratio": value.fail_ratio,
            "max_response_time": value.max_response_time,
            "median_response_time": value.median_response_time,
            "min_response_time": value.min_response_time,
            "num_failures": value.num_failures,
            "num_none_requests": value.num_none_requests,
            "num_requests": value.num_requests,
            "total_rps": value.total_rps,
        }

    # Footer
    data["request"].append("SUMMARY")
    data_new["SUMMARY"] = {}
    data["count"].append(sum_count)
    data_new["SUMMARY"]["num_requests"] = sum_count
    data_new["SUMMARY"]["num_failures"] = sum_failures
    if sum_count != 0:
        data["fail ratio"].append(sum_failures / sum_count)
        data_new["SUMMARY"]["fail_ratio"] = sum_failures / sum_count
        data["med resp time"].append(sum_total_response_time / sum_count)
        data_new["SUMMARY"]["median_response_time"] = (
            sum_total_response_time / sum_count
        )
        data["avg content lenght"].append(sum_total_content_length / sum_count)
        data_new["SUMMARY"]["avg_content_length"] = sum_total_content_length / sum_count
    else:
        data["fail ratio"].append(None)
        data_new["SUMMARY"]["fail_ratio"] = None
        data["med resp time"].append(None)
        data_new["SUMMARY"]["median_response_time"] = None
        data["avg content lenght"].append(None)
        data_new["SUMMARY"]["avg_content_length"] = None
    data_new["SUMMARY"]["total_content_length"] = sum_total_content_length
    data["total RPS"].append(sum_total_rps)
    data_new["SUMMARY"]["total_rps"] = sum_total_rps

    # Print table
    print(tabulate.tabulate(data, headers="keys", floatfmt=".3f"))

    print("Errors encountered:")
    errors = list(locust_stats.serialize_errors().values())
    if len(errors) == 0:
        print("Good, no errors.")
    else:
        errors_table = [
            e.update({"error": e["error"][:100]}) or e for e in copy.deepcopy(errors)
        ]
        table = tabulate.tabulate(errors_table, headers="keys")
        print(table)
    if status_data is not None:
        logging.debug("Adding errors to status data file")
        status_data.set("results.errors", errors)

    if not new_stats:
        # Add results to status data file
        transposed = {}
        for i in range(len(data["request"])):
            r_req = data["request"][i]
            r_status = "OK" if data["fail ratio"][i] == 0.0 else "EE"
            r = f"[{r_status}] {r_req}"
            if r in transposed:
                logging.error(
                    "Second same key? That is strange. We are loosing data in status data file."
                )
            transposed[r] = {}
            for f in ["count", "fail ratio", "med resp time", "total RPS"]:
                transposed[r][f] = data[f][i]
            sd_data = transposed
    else:
        if summary_only:
            sd_data = {k: v for k, v in data_new.items() if k == "SUMMARY"}
        else:
            sd_data = data_new

    if status_data is not None:
        logging.debug(
            f"Adding {'new' if new_stats else 'old'} style results to status data file"
        )
        status_data.set("results.requests", sd_data)

    return sum_failures
